# DKL

The official PyTorch implementation of the paper "Dual Knowledge Distillation Framework with Class-Adaptive Temperature and TopK Feature Perturbation for Few-Shot Prompt Learning".

# Overview
This repo contains the PyTorch implementation of DKL, described in the paper "Dual Knowledge Distillation Framework with Class-Adaptive Temperature and TopK Feature Perturbation for Few-Shot Prompt Learning".  
Due to the paper has not been accepted by any journal yet, we are not able to release the related source code for now, but only the pretrained models. We will continue to release the related source code after the paper has been accepted by a journal.


# Installation
This code is built upon [KgCoOp](https://github.com/htyao89/KgCoOp) . For environment setup and dataset preparation, please refer to the corresponding sections in their README.








# Acknowledgements
Our code is based on [KgCoOp](https://github.com/htyao89/KgCoOp) and [TCP](https://github.com/htyao89/Textual-based_Class-aware_prompt_tuning/) repository. We thank the authors for releasing their code. If you use our model and code, please consider citing these works as well.
